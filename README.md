# Airport Configuration Prediction Contest
The goal of this competion was to predict the runway configuration for the next six hours given a specific time and statistical data for ten airports. The statistical data provided includes airport traffic information, previous configurations at specific times, and weather forecast information. The weather forecast data includes forecast information for the next 24 hours, and the predictions are made/provided at every hour.

# Methodology
## Data Preparation
It is easier for aircraft to land when moving into the wind, and easier for aircraft to takeoff when the wind is behind them. The configuration of runways is typically selected based on the wind direction and speed for this reason. Also, the runway configuration does not usually change that often in a day. Because of these facts, the weather forecast information and the previous configuration are some of the best predictors for this competition.

When organizing the dataset into a useable form, it is important to ensure future information is not leaked to the model during training, as this information will not be available during inference. Therefore, the best possible weather forecast data and previous configuration **available at the prediction time** is saved for every unique prediction timestep and used as training data. The best possible weather data is found by first limiting the data to forecasts made in the past, and the forecast for the time closest to the target future time is used. The best previous configuration data is easier to find, as its the maximum timestep out of historic data. A dataset of this form is generated for each airport seperately.

## Model Selection
The prepared data was split at random into training and validation datasets. Several models were trained on the training dataset, and these models were compared by calculating the log loss metric for predictions made on the validation dataset. The best standalone model found using this method is the XGBoost model. Further, the XGBoost predictions on the validation data were improved by ensembling XGBoost with a Random Forest model with the predictions from XGBoost weighted higher. Finally, the parameters for these two models were individually tuned until the validation score converged.  
